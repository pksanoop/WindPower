#!/usr/bin/env python
# coding: utf-8

# <center><h1> Wind Power Forecasting </h1> </center>

# <center><h3> Sanoop Kammapata </h3> </center>

# ## Table of contents
# 1. [Introduction](#1.-Introduction)
# 2. [Data](#2.-Data)
# 3. [Develop Predictive Model](#3.-Develop-Predictive-Model)
# 

# ## 1. Introduction <a name="1.-Introduction"></a>

# The objective of this project is to predict wind power that could be generated from a windmill for the next 15 days using the previous data.

# ### 1.1 Load required libraries 

# In[1]:


# Import libraries
import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns
from pandas_profiling import ProfileReport
import datetime
import IPython
import IPython.display
import tensorflow as tf
get_ipython().run_line_magic('matplotlib', 'inline')
from windrose import WindroseAxes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.pipeline import Pipeline


# ## 2. Data <a name="2.-Data"></a>

# The data used in this project was collected from a Windmill and it has been recorded at a 10-minute interval from January 2018 till March 2020.

# ### 2.1 Load and explore the data 

# In[64]:


# Load data from CSV file
df = pd.read_csv('WindTurbineData.csv')
df


# In[65]:


# Explore data type
df.info()


# In[66]:


# Statistics of numerical data 
df.describe()


# In[67]:


# check for duplicates
df.duplicated().sum()   


# In[22]:


# Validate Blade2PitchAngle and Blade3PitchAngle are having same values and drop Blade3PitchAngle
if (df['Blade2PitchAngle'].equals(df['Blade3PitchAngle'])==True):
    df = df.drop('Blade3PitchAngle', axis=1)
    
# Drop ControlBoxTemperature as the value is zero 
df = df.drop('ControlBoxTemperature', axis=1) 


# In[23]:


df = df.fillna(method='ffill').fillna(method='bfill')
df.isnull().sum()


# ### 2.2 Data Visualization

# In[69]:


# Set the time column as the index so that is is very convenient to plot time series data
df = pd.read_csv('WindTurbineData.csv', parse_dates=['Unnamed: 0'],index_col=['Unnamed: 0'])
df.index = pd.to_datetime(df.index)        


# In[105]:


# Visualize missing values as a matrix
msno.matrix(df);


# In[119]:


# check the correlations between different numerical values
plt.figure(figsize=(18,16))
sns.heatmap(df.corr(),square=True,annot=True,linewidths=0.1,cmap="coolwarm")
plt.show();


# In[78]:


# Daily power output 
df['ActivePower'].plot()
plt.title('Power vs Day')
plt.ylabel('Power Generated', fontsize=10)


# In[114]:


# Resampling the mean daily values to view a clear data
plt.figure(figsize=(10, 5))
df['ActivePower'].resample('D').mean().plot()
plt.title('Average Power per Day')
plt.ylabel('Mean Power Generated (kW)', fontsize=10);
plt.xlabel('Period', fontsize=10);


# In[115]:


# Resampling daily mean wind
plt.figure(figsize=(10, 5))
df['WindSpeed'].resample('D').mean().plot(color='green')
plt.title('Average Wind Speed Per Day')
plt.ylabel('Wind Speed (m/s)', fontsize=10)
plt.xlabel('Period', fontsize=10);


# In[95]:


# Correlation of Active power and Wind 
plt.figure(figsize=(10, 5))
df['ActivePower'].resample('D').mean().plot(legend=True)
(df['WindSpeed']).resample('D').mean().plot(legend=True)
plt.ylabel('Active Power and Wind speed', fontsize=10)
plt.xlabel('Period', fontsize=10);


# In[97]:


# Correlation of Active power and Wind (Normalize wind by multiplying with 100)
plt.figure(figsize=(10, 5))
df['ActivePower'].resample('D').mean().plot(legend=True)
(df['WindSpeed']*100).resample('D').mean().plot(legend=True)
plt.title('Average Power and Wind Speed Per Day')
plt.ylabel('Active Power and Wind speed', fontsize=10)
plt.xlabel('Period', fontsize=10);


# In[116]:


# Power generated by month
plt.figure(figsize=(10, 5))
sns.boxplot(x=df.index.month_name(), y='ActivePower', data=df) 
plt.ylabel('Active Power (kW)', fontsize=10)
plt.xlabel('Month', fontsize=10)
plt.xticks(rotation=45)
plt.title('Power Generated by Month', fontsize=15);


# In[117]:


# Wind Speed generated by month
plt.figure(figsize=(10, 5))
sns.boxplot(x=df.index.month_name(), y='WindSpeed', data=df)
plt.ylabel('Wind Speed (m/s)', fontsize=10)
plt.xlabel('Month', fontsize=10)
plt.xticks(rotation=45)
plt.title("Monthly Wind Speed", fontsize=15);


# In[118]:


# Wind speed generated by hour
plt.figure(figsize=(10, 5))
sns.boxplot(x=df.index.hour, y='WindSpeed', data=df)
plt.ylabel('WindSpeed (m/s)', fontsize=10)
plt.xlabel('Hour', fontsize=10)
plt.title("Hourly Wind Speed", fontsize=15);


# In[112]:


from windrose import WindroseAxes

plt.figure(figsize = (8, 6))
ax = WindroseAxes.from_ax()
ax.bar(df.WindDirection, df.WindSpeed, normed=True, opening= 0.8, edgecolor='white')
ax.set_legend(loc = "best")
plt.title('Wind Direction VS Wind Speed')
plt.show()


# ### 2.3 Feature engineering 

# In[24]:


# Rename the date column  
df.rename(columns={'Unnamed: 0': 'date_column'}, inplace=True)

# Select the required features (Dropped other features based on heat map)
df = df[['date_column', 'ActivePower', 'WindSpeed', 'GeneratorRPM', 'ReactivePower', 'RotorRPM', 'AmbientTemperatue', \
                 'WindDirection', 'Blade1PitchAngle', 'Blade2PitchAngle', 'HubTemperature', 'MainBoxTemperature', 'GearboxBearingTemperature', \
                 'GearboxOilTemperature']].copy()


# In[25]:


# crete a new column called Weekday
df['weekday'] = df['date_column'].dt.dayofweek
# get one hot encoding
one_hot_coding = pd.get_dummies(df['weekday'])
df = df.join(one_hot_coding)
df = df.drop('weekday', axis=1)
date = pd.to_datetime(df.pop('date_column'))
df.head()


# In[26]:


# convert datetime column to seconds
timestamp_s = date.map(datetime.datetime.timestamp)


# In[27]:


# Get usable signals by using sine and cosine transforms to clear "Time of day" and "Time of year" signals
day = 24*60*60
year = (365.2425)*day

df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))
df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))
df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))
df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))


# In[28]:


plt.plot(np.array(df['Day sin'])[:200])
plt.plot(np.array(df['Day cos'])[:200])
plt.xlabel('Time [h]')
plt.title('Time of day signal');


# ## 3. Develop Predictive Model <a name="3.-Develop-Predictive-Model"></a>

# From literature, it was found that LSTM is one of the best models for wind power forecasting. TensorFlow was used for this project and it was based on the introduction to time series forecasting using TensorFlow tutorial (https://www.tensorflow.org/tutorials/structured_data/time_series#feature_engineering). Different styles of models including Convolutional and Recurrent Neural Networks (CNNs and RNNs) were tried.

# ### 3.1 Split the data

# We will use a (70%, 20%, 10%) split for the training, validation, and test sets. Note the data is not being randomly shuffled before splitting. This is for two reasons:
# - It ensures that chopping the data into windows of consecutive samples is still possible.
# - It ensures that the validation/test results are more realistic, being evaluated on the data collected after the model was   
#   trained.

# In[29]:


column_indices = {name: i for i, name in enumerate(df.columns)}

n = len(df)
train_df = df[0:int(n*0.7)]
val_df = df[int(n*0.7):int(n*0.9)]
test_df = df[int(n*0.9):]

num_features = df.shape[1]


# ### 3.2 Normalize the data

# In[30]:


# Normalize the data
train_mean = train_df.mean()
train_std = train_df.std()

train_df = (train_df - train_mean) / train_std
val_df = (val_df - train_mean) / train_std
test_df = (test_df - train_mean) / train_std


# In[31]:


# Visualize the normalized data
df_std = (df - train_mean) / train_std
df_std = df_std.melt(var_name='Column', value_name='Normalized')
plt.figure(figsize=(12, 6))
ax = sns.violinplot(x='Column', y='Normalized', data=df_std)
_ = ax.set_xticklabels(df.keys(), rotation=90)


# ### 3.3 Data windowing

# The models in this tutorial will make a set of predictions based on a window of consecutive samples from the data.
# The main features of the input windows are:
# - The width (number of time steps) of the input and label windows.
# - The time offset between them.
# - Which features are used as inputs, labels, or both.

# In[35]:


class WindowGenerator():
    def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=test_df,
               label_columns=None):
        # Store the raw data.
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df

        # Work out the label column indices.
        self.label_columns = label_columns
        if label_columns is not None:
            self.label_columns_indices = {name: i for i, name in
                                        enumerate(label_columns)}
        self.column_indices = {name: i for i, name in
                               enumerate(train_df.columns)}

        # Work out the window parameters.
        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift

        self.total_window_size = input_width + shift
        self.input_slice = slice(0, input_width)
        self.input_indices = np.arange(self.total_window_size)[self.input_slice]

        self.label_start = self.total_window_size - self.label_width
        self.labels_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

    def __repr__(self):
        return '\n'.join([
            f'Total window size: {self.total_window_size}',
            f'Input indices: {self.input_indices}',
            f'Label indices: {self.label_indices}',
            f'Label column name(s): {self.label_columns}'])


# In[36]:


# We want to predict 15 days into the future.
w2 = WindowGenerator(input_width=30*24*6, label_width=15*24*6, shift=15*24*6,
                     label_columns=['ActivePower'])

w2


# In[37]:


# Given a list of consecutive inputs, the split_window method will convert them to a window of inputs and a window of labels.
def split_window(self, features):
    inputs = features[:, self.input_slice, :]
    labels = features[:, self.labels_slice, :]
    if self.label_columns is not None:
        labels = tf.stack(
            [labels[:, :, self.column_indices[name]] for name in self.label_columns],
            axis=-1)

  # Slicing doesn't preserve static shape information, so set the shapes
  # manually. This way the `tf.data.Datasets` are easier to inspect.
    inputs.set_shape([None, self.input_width, None])
    labels.set_shape([None, self.label_width, None])

    return inputs, labels

WindowGenerator.split_window = split_window


# In[38]:


# Stack three slices, the length of the total window:
example_window = tf.stack([np.array(train_df[:w2.total_window_size]),
                           np.array(train_df[100:100+w2.total_window_size]),
                           np.array(train_df[200:200+w2.total_window_size])])


example_inputs, example_labels = w2.split_window(example_window)

print('All shapes are: (batch, time, features)')
print(f'Window shape: {example_window.shape}')
print(f'Inputs shape: {example_inputs.shape}')
print(f'labels shape: {example_labels.shape}')


# In[39]:


w2.example = example_inputs, example_labels


# In[42]:


# Plot method allows a simple visualization of the split window
def plot(self, model=None, plot_col='ActivePower', max_subplots=5):
    inputs, labels = self.example
    plt.figure(figsize=(70, 20))
    plot_col_index = self.column_indices[plot_col]
    max_n = min(max_subplots, len(inputs))
    for n in range(max_n):
        plt.subplot(max_n, 1, n+1)
        plt.ylabel(f'{plot_col} [normed]')
        plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             label='Inputs', marker='.', zorder=-10)

        if self.label_columns:
            label_col_index = self.label_columns_indices.get(plot_col, None)
        else:
            label_col_index = plot_col_index

        if label_col_index is None:
            continue

        plt.scatter(self.label_indices, labels[n, :, label_col_index],
                edgecolors='k', label='Labels', c='#2ca02c', s=124)
        if model is not None:
            predictions = model(inputs)
            plt.scatter(self.label_indices, predictions[n, :, label_col_index],
                  marker='X', label='Predictions',
                  c='#ff0000', s=154)

        if n == 0:
            plt.legend(prop={'size': 20})

        plt.xlabel('Time [20 min]')

WindowGenerator.plot = plot


# In[43]:


w2.plot()


# In[44]:


# Make_dataset method will take a time series DataFrame and convert it to a tf.data.Dataset of (input_window, label_window) pairs using the tf.keras.utils.timeseries_dataset_from_array function.

def make_dataset(self, data):
    data = np.array(data, dtype=np.float32)
    ds = tf.keras.preprocessing.timeseries_dataset_from_array(
        data=data,
        targets=None,
        sequence_length=self.total_window_size,
        sequence_stride=1,
        shuffle=True,
        batch_size=120,)

    ds = ds.map(self.split_window)

    return ds

WindowGenerator.make_dataset = make_dataset


# The WindowGenerator object holds training, validation, and test data.
# Add properties for accessing them as tf.data.Datasets using the make_dataset method you defined earlier. Also, add a standard example batch for easy access and plotting.

# In[47]:


@property
def train(self):
    return self.make_dataset(self.train_df)

@property
def val(self):
    return self.make_dataset(self.val_df)

@property
def test(self):
    return self.make_dataset(self.test_df)

@property
def example(self):
    """Get and cache an example batch of `inputs, labels` for plotting."""
    result = getattr(self, '_example', None)
    if result is None:
        result = next(iter(self.train))
    # And cache it for next time
        self._example = result
    return result

WindowGenerator.train = train
WindowGenerator.val = val
WindowGenerator.test = test
WindowGenerator.example = example


# In[48]:


# Each element is an (inputs, label) pair
w2.train.element_spec


# In[49]:


for example_inputs, example_labels in w2.train.take(1):
    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
    print(f'Labels shape (batch, time, features): {example_labels.shape}')


# In[50]:


wide_window = WindowGenerator(
    input_width=2*6, label_width=2*6, shift=1,
    label_columns=['ActivePower'])

wide_window


# In[51]:


# package the training into a function
MAX_EPOCHS = 20

def compile_and_fit(model, window, patience=3):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=3,
                                                    mode='min')

    model.compile(loss=tf.losses.MeanAbsoluteError(),
                optimizer=tf.optimizers.Adam(lr=0.01),
                metrics=[tf.metrics.MeanAbsoluteError()])

    history = model.fit(window.train, epochs=MAX_EPOCHS,
                      validation_data=window.val,
                      callbacks=[early_stopping])
    return history


# In[52]:


# Multi-step prediction in whuch the model needs to learn to predict a range of future values. A sequence of future values are predicted.

OUT_STEPS = 15*24*6
multi_window = WindowGenerator(input_width=15*24*6,
                               label_width=OUT_STEPS,
                               shift=OUT_STEPS)

multi_window.plot()
multi_window


# ### 3.4 Baseline model

# In[54]:


class MultiStepLastBaseline(tf.keras.Model):
    def call(self, inputs):
        return tf.tile(inputs[:, -1:, :], [1, OUT_STEPS, 1])

last_baseline = MultiStepLastBaseline()
last_baseline.compile(loss=tf.losses.MeanAbsoluteError(),
                      metrics=[tf.metrics.MeanAbsoluteError()])

multi_val_performance = {}
multi_performance = {}

multi_val_performance['Last'] = last_baseline.evaluate(multi_window.val)
multi_performance['Last'] = last_baseline.evaluate(multi_window.test)
multi_window.plot(last_baseline)


# In[55]:


# Repeat Baseline model
class RepeatBaseline(tf.keras.Model):
    def call(self, inputs):
        return inputs

repeat_baseline = RepeatBaseline()
repeat_baseline.compile(loss=tf.losses.MeanAbsoluteError(),
                        metrics=[tf.metrics.MeanAbsoluteError()])

multi_val_performance['Repeat'] = repeat_baseline.evaluate(multi_window.val)
multi_performance['Repeat'] = repeat_baseline.evaluate(multi_window.test)
multi_window.plot(repeat_baseline)


# ### 3.5 Linear model

# In[56]:


OUT_STEPS = 15*24*6
multi_window = WindowGenerator(input_width=30*24*6,
                               label_width=OUT_STEPS,
                               shift=OUT_STEPS)

multi_window.plot()
multi_window


# In[57]:


multi_linear_model = tf.keras.Sequential([
    # Take the last time-step.
    # Shape [batch, time, features] => [batch, 1, features]
    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),
    # Shape => [batch, 1, out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

history = compile_and_fit(multi_linear_model, multi_window)

#IPython.display.clear_output()
multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)
multi_performance['Linear'] = multi_linear_model.evaluate(multi_window.test)
multi_window.plot(multi_linear_model)


# In[58]:


# Dense: Before applying models that actually operate on multiple time-steps, it's worth checking the performance of deeper, more powerful, single input step models.

multi_dense_model = tf.keras.Sequential([
    # Take the last time step.
    # Shape [batch, time, features] => [batch, 1, features]
    tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),
    # Shape => [batch, 1, dense_units]
    tf.keras.layers.Dense(512, activation='relu'),
    # Shape => [batch, out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

history = compile_and_fit(multi_dense_model, multi_window)

multi_val_performance['Dense'] = multi_dense_model.evaluate(multi_window.val)
multi_performance['Dense'] = multi_dense_model.evaluate(multi_window.test)
multi_window.plot(multi_dense_model)


# ### 3.6 Convolution neural network

# In[59]:


CONV_WIDTH = 3
multi_conv_model = tf.keras.Sequential([
    # Shape [batch, time, features] => [batch, CONV_WIDTH, features]
    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),
    # Shape => [batch, 1, conv_units]
    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),
    # Shape => [batch, 1,  out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

history = compile_and_fit(multi_conv_model, multi_window)

multi_val_performance['Conv'] = multi_conv_model.evaluate(multi_window.val)
multi_performance['Conv'] = multi_conv_model.evaluate(multi_window.test)
multi_window.plot(multi_conv_model)


# ### 3.7 Recurrent neural network

# In[ ]:


multi_lstm_model = tf.keras.Sequential([
    # Shape [batch, time, features] => [batch, lstm_units]
    # Adding more `lstm_units` just overfits more quickly.
    tf.keras.layers.LSTM(32, return_sequences=False),
    # Shape => [batch, out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros()),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])

history = compile_and_fit(multi_lstm_model, multi_window)

multi_val_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.val)
multi_performance['LSTM'] = multi_lstm_model.evaluate(multi_window.test)
multi_window.plot(multi_lstm_model)


# 

# In[ ]:




